{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f978eff9-877f-4100-99d4-dea0d6b949b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e96582-e135-4586-8da8-92c5ddfc8d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "##1. Check if GPU is available, if not use cpu\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70972211-bc30-435a-a85d-4b7b39d645fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for emotion contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/emotion\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"emotion\", split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c39c28-6230-43e1-8019-b1572d02c540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>i could feel her whimper to the thought of bei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>im certainly not going to sit and tell you wha...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>im sorry that there wasnt more humor in this p...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>i feel ive got my foot in the door of the fant...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>i say whatever comes in my mind tell you direc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0                              i didnt feel humiliated      0\n",
       "1    i can go from feeling so hopeless to so damned...      0\n",
       "2     im grabbing a minute to post i feel greedy wrong      3\n",
       "3    i am ever feeling nostalgic about the fireplac...      2\n",
       "4                                 i am feeling grouchy      3\n",
       "..                                                 ...    ...\n",
       "155  i could feel her whimper to the thought of bei...      0\n",
       "156  im certainly not going to sit and tell you wha...      5\n",
       "157  im sorry that there wasnt more humor in this p...      5\n",
       "158  i feel ive got my foot in the door of the fant...      1\n",
       "159  i say whatever comes in my mind tell you direc...      3\n",
       "\n",
       "[160 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb27830f-8e02-4639-a00f-39219f0beb3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length=512):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            # Encode the inputs\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>',\n",
    "                                       truncation=True, \n",
    "                                       max_length=max_length, \n",
    "                                       padding=\"max_length\")\n",
    "            \n",
    "            self.input_ids.append(encodings_dict['input_ids'])\n",
    "            self.attn_masks.append(encodings_dict['attention_mask'])\n",
    "            # For language modeling, labels are input_ids shifted by one\n",
    "            self.labels.append(encodings_dict['input_ids'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {'input_ids': torch.tensor(self.input_ids[idx]),\n",
    "                'attention_mask': torch.tensor(self.attn_masks[idx]),\n",
    "                'labels': torch.tensor(self.labels[idx])}\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68247d27-5b39-4973-b16e-a2e66e4e498b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Set the padding token to the EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ef776b1-c113-4081-b3d4-a90d8e010bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 124.439808 M\n",
      "Trainable Parameters: 124.439808 M\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters()) / 1000000\n",
    "\n",
    "# Count the number of trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000000\n",
    "\n",
    "print(f\"Total Parameters: {total_params} M\")\n",
    "print(f\"Trainable Parameters: {trainable_params} M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "536e23b4-58aa-489f-b8b3-1f3fecfbcd31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = EmotionDataset(dataset['text'], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51d71b66-21cb-4274-8d9f-6254f9767f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_finetuned_emotion\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,  # For demonstration, keep it to 1\n",
    "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
    "    save_steps=1000,\n",
    "    save_total_limit=1,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f8cb991-d96c-493a-a3e9-5ed0dd59746b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 17:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.178500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=0.870284378528595, metrics={'train_runtime': 1110.25, 'train_samples_per_second': 0.144, 'train_steps_per_second': 0.036, 'total_flos': 41806725120000.0, 'train_loss': 0.870284378528595, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dd18fd5-1440-45fd-be78-6ddee87931d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I felt so happy and joyful.\n",
      "Emotion: It was wonderful to see so many people get the support of so they were able to communicate with the idea of what to do and to enjoy while still in that feeling of so much happiness\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Text: I felt so happy and joyful.\\nEmotion:\"\n",
    "\n",
    "# Generate completion\n",
    "completion = pipe(prompt, max_length=50, num_return_sequences=1)\n",
    "print(completion[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec4c28-dda5-474a-9dd9-572c87b009ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed3eaee-69b5-4462-a0f7-fbc65f67fee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bf3eb-e4d2-45e3-969a-a92546bbb95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052030c-b888-4628-8bf8-9f836128399f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
